 **Naturalness**. Try to build a benchmark that has natural questions that some category of humans ask on a frequent basis. Otherwise, who is the benchmark for and why should anyone be excited about progress on the benchmark. Another way to think about fulfilling naturalness is considering usefulness: would a system that got better-than-baseline accuracy be useful to humans?

**Automatically evaluateable**. Certain fields have stagnated due to difficulty of evaluation, summarization is a good example of this. It is difficult to automatically evaluate a answer.

**Challenging**. If you launch an automatically evaluatable and natural benchmark, but the accuracy of the best LM at launch is 80%, people will see your benchmark as already being _solved_ and won’t want to try and build models to improve performance on it. Given the rapid speed of progress in the field, the "acceptable" top score on release has quickly gone from < 30% -> 0% accuracy. The author says: "_Edit, May 2025: Sorry but I have to make another edit. Due to the speed of development of AI I’m now asking my collaborators, not to think of benchmarks that would have benchmarks achieving 0% at launch, but to think of benchmarks that would have models achieving “-200%” at launch. Find questions that are so hard that even if the models improve 3x they’ll still get zero. Just building a benchmark where models get 0% today might not be enough anymore. You have to look at how the models have been improving over the past 3-6 months, try to predict where they’ll be in 6-12 months and build benchmarks that would not only make current models fail, but benchmarks that would make the models of next year fail as well. Anything easier than that might get saturated much more quickly than you expect._

**Leakage**. Build benchmarks that would be hard to leak into the training data, are there benchmarks where even if the training data is exposed to the LM, it is still difficult? 

Have **one** number that people aim for, e.g. 80% accuracy on X. Don't use multiple metrics, don't divide accuracy by category. Just one overall number.