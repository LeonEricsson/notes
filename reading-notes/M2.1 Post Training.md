---
source: https://www.minimax.io/news/post-training-experience-and-insights-for-agent-models
---


The amount of infra being deployed for post training environment creation is becoming very clear. Labs need to dedicate extensive resources to both curate such environemnts, and deploy them at scale during training. On the curation side of things, M2 scrapes GitHub repos for PRs, and from them builds SWE-style datasets spanning 10 major programming languages and a wide range of tasks such as feature implementation, bug fixing, performance optimization, refactoring. All of these require individual handling, turning a PR into a environment. This process results in 140,000+ verifiable tasks spanning 10,000+ runnable PR environments. Each programming language has different build requirements, and creating an environment requires building the repository at the exact moment of the PR.
There's also the need to serve these environments during training, this is an increasingly difficult problem in the age of long horizon agents trained across a multitude of different environment types which require different underlying systems to support said environments.

---

They perform web search training by generating synthetic long horizon datasets based on a "explore and evolve" approach. They first use agents to explore the web to construct information-rich seed questions, these queries are then evolved iteratively to increase complexity. For example an agent starts from a seed like "Brazil national football team”, then discovers the 1950 World Cup, the Maracanazo Blow, referee George Reader, his later role as an English club chairman, and the 1976 FA Cup final, which was won by Bobby Stoke's goal. It then uses this trajectory to synthesize a information rich intial query, while obfuscating, removing and substituting facts that make retrieval too easy. This may include things like 

- Converting specific match details into a vague description such as “a World Cup with a unique format and no knockout stage
- Replacing salient information like “defeating Manchester United in the FA Cup” with “leading a second-division club to victory over a top-tier powerhouse”, thereby increasing retrieval difficulty
- Removing easily searchable facts, such as a player’s age at the time of death, which can be readily found on Wiki.

---

They still use CISPO (introduced in MiniMax M1). A short recap on CISPO. It's a REINFORCE style algorithm that's augmented with a importance sampling correction to correct for off policy settings, meaning it, unlike REINFORCE and like PPO/GRPO, can be used in off-policy settings like a large rollout batch followed by mini-batch updates. The important difference from PPO style clipping is that CISPO **only clips the importance weights**, rather than clipping the entire surrogate objective like PPO. In PPO's case, clipping gradients leads to a 0 gradient. CISPO on the other hand, which only clips the importance weight, means every token still receives a gradient signal proportional to its advantage—the weight just bounds how much that signal is amplified or suppressed by the importance ratio. This is a bias-variance tradeoff: clipping weights introduces bias but controls variance and, critically, avoids dropping token gradients entirely.

They still retain the LM Head FP32 fix that was introduced to address training-inference mismatch. However, they add to their reportiore of things to address this by also using masked importance sampling, and what they call "PPO-based trajectory filtering" which is unclear, but overall its basically just filtering out sequences that are too off policy.