https://arxiv.org/abs/2601.16725

Builds on LongCat-Flash-Chat, a 560B-A27B MoE.


### pre-train / mid-train
Mid-training with 500B tokens to a 32k/128k stage, and 40B to a 256k stage. They add to the existing mid-triaining by introdocing a bunch of agent-trajectories, especially involving long-horizon trajectories with reasoning, planning and interaction. Much of this is synthetic data, or atleast "partly synthetic". They mine naturally long text corpora such as instructions, multi step problems, tutorials, and turn them into explict agent interaction trajectories. This partly includes introducing tool calling into the trajectories. To sumplement this text material, they also synthesize trajectoris based on environments, meaning they use real executable environments to construct agentic data.  They implement lightweight Python environments for collected toolsets and generate trajectories through controlled tool-chain sampling and execution
verification. 

### rl
first, procure rl data. the biggest "hype" right now is to overindex on coding and terminal like environments. 

**code sandbox**. they build a scalable coding sandbox system that provides unified tool interfaces with high-throughput interaction. they incorporate common things like search, file reading and writing, code editing and shell execution into a standardized environment interface.

**agentig tool use environment**.  they design a fully automated pipeline that converts a high level domain specification into an executable graph. 

#### task set
A common practice is to first construct a diverse collection of environments paired with tasks instructions spanning multiple domains and difficulty levels, and then assess task suitability by evaluating the modelâ€™s pass rate. Based on this signal, tasks that are neither trivially solvable nor prohibitively hard are selected for reinforcement learning. However, such tasks are not available across domains. For something like coding, there already exists complex and high quality tasks suitable for this, so they just collect and curate these. However for things like agentic search and tool use, suitable tasks are scarce. They propose yet another synthesis pipeline to construct such tasks spanning complexity levels for agentic search and tool-use scenarios. For agentic search, this means trying to build relational graphs from wikipedia entries, then using these graphs to generate question answer pairs. 

#### dora

The high-level overview of the async framework:

![[Pasted image 20260204112656.png|900]]

This is a fully streaming pipeline where finished samples are streamed to queue in a producer-consumer design. Within the rollout loop in the RolloutManager, they remove batch barriers, enabling LLM generation, environment execution, and reward computation to be executed on remote workers at the granularity of individual samples. This prevents accelerator idling while waiting for batched ENV calls to complete. Naturally, this async pipeline enabled multi-version async training where trajectories generated by different model versions are immediately enqueued upon completion. 

They employ Prefill-Decode Disaggregation, which means splitting inference work into two separate device groups, allowing the decode execution graph to run without beinf interrupted by the prefill workloads of newly arriving requests.  

![[Pasted image 20260204113152.png|800]]


#### rl training strategy

they employ **GSPO** from Qwen as their training objective, attributing this choice to empirical effectivness on MoE models, and more stable sequence-level optimization for long-horizon agentic scenarios.

curriculum learning is employed based on task difficulty which is quantified using model pass rate prior to optimization. they also take extra care of agentic tasks, which are categorized by the agentic capability they primarily excercise such as basic tool invocation, multi-step planning or automomous decision making. The curriculm applied is tasks that are easier to learn or expose capabilities that are expected to be
autonomously reused by the agent when solving harder task, are introduced first, then as training progresses they shift towards taks that require more advanced combinations of agentic capabilities. This means models first learn reusable agentic skills, and then compose them to solve increasingly complex problems. 

they also try to employ a dynamic budget allocation, where the rollout budget is not uniformly distributed to tasks in a batch, but rather dynamically assigned based on some value function.

self-verification. the authors observe a notable asymmetry: even advanced reasoning models that are capable of generating high-quality trajectories often struggle to reliably assess the correctness of those trajectories in the absence of explicit ground-truth signals. To enhance this capability, they periodically perform self-verification phases during training. 

agentic context management. in agentic multi-turn trajectories that interleave model reasoning and tool invocation, the number of interactions grow significantly, between both reasoning and tool responses. The totel context length quickly becomes unmanagable, leading to context rot or even context window overflow. To address this, they combine summary-based context management and discard based management. Summary based context management simply means asking the model to summarize the context. After ablation experiments, they land on 80k tokens as a good spot to invoce this. Discard-based just means that when the ctx length exceeds a predefined threshhold, the model will discard entire or partial historical context and then resume or restart the generation process. 

compare this to the ds v3.2 strategy: strictly for tool calling processes they only discard historical **reasoning** content when a new user message is introduced in the conversation. If only tool related messages are appended, the reasoning content is retained throughput the interaction. When reasoning traces are removed, the history of tool calls and their results remains preserved in the context. Once the token usage exceeds 80% of the context window length, DSV3.2 advocats the discard all strategy which resets the context by discarding all previous tool cool history. On BrowserComp this outperforms other strategies. 

LongCatFlash combines discard all and summary management into a hybrid solution.  Specifically, they first apply summary-based compression whenever the context window exceeds our predefined limit of 80K tokens. When the interaction exceeds the maximum number of turns, they trigger a discard-all reset and restart generation with an initialized system and user prompt derived from the original question.

**multi-domain training** they employ multi domain training, arguing that this benefits from generalization beyond domain specific patterns. to ensure stable optimization under highly diverse task distributions they jointly optimize across diverse environments within each training batch. It is necessary to ensure that all domains contribute comparably to the training process, and prevent any single training batch from being dominated by a small subset of domains. This algorithm constraint significantly degrades the efficiency of our DORA system, as it breaks the design principle of asynchrony: the trainer may be forced to wait for slow or rare long-tailed domains to produce enough samples, while faster domains accumulate excessive rollout trajectories, leading to scheduling bubbles and device underutilization. To mitigate this, they oversample from such domanis. 