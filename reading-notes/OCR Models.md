| Model                      | Total params (active if MoE) | Vision/encoder params + LM/decoder params                        | OmniDocBench V1.5 (EN+ZH)                           | olmoOCR-Bench                                                                                                  | Other benchmarks                    | Cost/throughput + hardware                                                       | Notes                                                                                                                                                                                                                                                                             |
| -------------------------- | ---------------------------: | ---------------------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------- | -------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| GLM-OCR (2026-02)          |                         0.9B | Cog-ViT visual encoder (params N/A)<br>GLM-0.5B language decoder | Overall: 94.62<br>                                  |                                                                                                                | OCRBench (Text): 94.0               | PDF: 1.86 pages/s<br>images: 0.67 images/s                                       | CogViT visual encoder pre-trained from scratch, a lightweight cross-modal connector, and a GLM-0.5B language decoder                                                                                                                                                              |
| PaddleOCR-VL-1.5 (2026-01) |                         0.9B | Visual Encoder (params N/A)<br>ERNIE-4.5-0.3B LM                 | Overall: 94.50<br>Formulas: 94.21<br>Tables: 92.76  |                                                                                                                |                                     | 1.43 pages/s <br>2016.6 tokens/s <br>on single NVIDIA A100;                      |                                                                                                                                                                                                                                                                                   |
| LightOnOCR-2               |                           1B | Mistral Small 3.1 ViT (~400M)<br>Qwen3 0.6B                      |                                                     | 83.2 (headers/footers category excluded)                                                                       |                                     | 5.71 pages/s (NVIDIA H100)<br><10$ per 1M pages                                  | Self-curated pre-training mix with a focus on documents in European languages. During training they train with high resolution (<1540px). They apply RLVR loop following olmOCR (GRPO) to target persistent failure modes that are difficult to address with supervised learning. |
| DeepSeek-OCR 2 (2026-01)   |       ~3.58B (~1.08B active) | DeepEncoder V2  580M <br>DeepSeek MoE 3B (500M active)           | Overall: 91.09<br>Formulas: 90.31<br>Tables: 87.75  | 76.3 (headers/footers category excluded)                                                                       |                                     |                                                                                  | Vision encoder is trained from scratch. LM is a 3B DeepSeek MoE. First encoder is pretrained, then the both are trained jointly, then  DeepEncoder is frozen and only LM is trained.                                                                                              |
| olmOCR-7B-1025 (2025-10)   |                           7B | Qwen2-VL-7B-Instruct fine-tune                                   | Overall: 81.79<br>Formulas: 86.04 <br>Tables: 68.92 | 75.5 (anchored)<br>82.3 (olmo-pipeline: rotates, renders, retries)<br>80.4 (headers/footers category excluded) | <br>                                | $176 per 1M PDF pages <br>11.4 pages/sec and 130 tokens/sec <br>(on single H100) | SFT + RL fine-tune on top of Qwen2-VL-7B                                                                                                                                                                                                                                          |
| dots.ocr (2025-12)         |                   2.9B total | Vision Encoder: 1.2B<br>LM Decoder: 1.7B                         | Overall: 88.41<br>Formula: 83.22<br>Tables: 86.78   | 79.1<br>76.9 (headers/footers category excluded)                                                               | XDocParse: 82.3 <br>(126 languages) |                                                                                  | Vision encoder trained from scratch. The LM is initialized from Qwen2.5-1.5B, then they are jointly SFT trained. Introduces XDocParse which is the first large scale multilingual (beyond EN+ZH) bench.                                                                           |

## Model notes

### GLM-OCR (26-02)

GLM-OCR is a compact (0.9B) VLM-style OCR system built on a GLM-V encoder–decoder backbone: a CogViT visual encoder (pre-trained on large-scale image–text), a lightweight cross-modal connector with token downsampling, and a GLM-0.5B language decoder.  Training-wise, the model card highlights Multi-Token Prediction (MTP) loss plus “stable full-task” reinforcement learning to improve efficiency, accuracy, and generalization. Operationally it’s positioned as a two-stage document pipeline: (1) layout analysis and (2) parallel recognition, explicitly calling out PP-DocLayout-V3 as the layout backbone.A  practical detail that matters for integration: GLM-OCR distinguishes document parsing prompts (text/formula/table extraction) from information extraction prompts, where outputs must follow a strict JSON schema for downstream processing. 

### PaddleOCR-VL-1.5 (26-01)

PaddleOCR-VL-1.5 is framed as a robust two-stage document parsing stack: PP-DocLayoutV3 first performs layout analysis (including multi-point localization and reading order), then PaddleOCR-VL-1.5-0.9B recognizes localized regions across modalities (text, tables, formulas, charts, seals).  A key PP-DocLayoutV3 change vs earlier versions is shifting to an instance-segmentation, end-to-end Transformer that merges deting order prediction (built on RT-DETR with a mask head).  The recognizer keeps a lightweight 0.9B design (Native Resolution Visual Encoder + Adaptive MLP Connector + ERNIE-4.5-0.3B LM) and e formula, table, chart, seal, and text spotting, with explicit attention to “hard” cases like rare characters, ancient Chinese, multilingual tables, and text decorations.  Training emphasizes real-world robustness: PP-DocLayoutV3 is trained on 38k manually annotated document samples and uses disto simulate mobile-photo deformations.  For the VLM stage, the report describes a progressive multi-stage recipe (pre-training + instruction fine-tuning + an RL stage): image–text pairs, instruction fine-tuning adds new capabilities (notably seal recognition and text spotting), and an RL stage uses GRPO to reduce style inconsistency and prioritize high-value samples.  Data curation is a major pillar: instruction FT uses Uncertainty-Aware Cluster Sampling (UACS) (CLItsters, alongside hard-case mining to broaden coverage of non-standard layouts.

### LightOnOCR-2-1B (26-01)

LightOnOCR-2-1B is a 1B-parameter, end-to-end multilingual VLM that converts rendered document pages (PDFs/scans/images) directly into clean, naturally ordered text, explicitly positioned as an alternative to brittle multi-stage OCR pipelines. Architecturally, the paper describes three core blocks: (1) a native-resolution ViT vision encoder initialized from Mistral-Small-3.1’s vision encoder weights, (2) a 2-layer MLP multimodal projector with spatial merging (×2) to reduce visual token count, and (3) a Qwen3-initialized language decoder that emits a single linearized page representation. 

Training-wise, LightOnOCR-2 keeps the same overall architecture as v1 but scales the pretraining mixture (the paper describes scaling from 17M → 43M pages) and emphasizes stronger scan/scientific/French coverage plus higher-resolution training (max longest edge 1540 px). Post-training uses RLVR (Reinforcement Learning with Verifiable Rewards) to target hard failure modes (notably repetition loops and formatting/math consistency constraints enforced via checks), and the model family includes bbox-capable variants refined with IoU-based RLVR for embedded-image localization. On evaluation, the paper reports 83.2 overall on OlmOCR-Bench (headers/footers excluded in their main table) and throughput of 5.71 pages/sec on a single NVIDIA H100 (80GB). The HF model card additionally frames this as roughly 493k pages/day at <$0.01 per 1,000 pages (for their stated setup).

The training mix is European / Latin. Unfortunately this means that the model isn't evaluated on OmniDocBench v1.5 like almost every other OCR model (because ODC is EN+ZH).

### DeepSeek-OCR 2 (26-01)

DeepSeek-OCR 2 is framed as an “encoder-first” refresh: it keeps the existing MoE decoder and puts the novelty into how images get turned into an ordered, decoder-friendly sequence. The core idea is DeepEncoder V2, which introduces “causal flow” queries and a hybrid attention scheme: visual tokens keep bidirectional attention (**ViT**-like), while appended query tokens use causal attention (decoder-like) to encourage an ordering that better matches how documents are read. That design is explicitly motivated by the mismatch between flattening 2D layouts into a 1D sequence and the non-linear reading patterns in forms/tables/layout-rich pages. The paper also emphasizes practical readiness: in production, where they can’t score against ground truth, they track repetition rate and report reductions for both online images and batch PDF processing.

### olmOCR (25-10)
olmOCR is best read as an OCR model plus a production-oriented PDF linearization toolkit: the focus is extracting clean, linearized text in natural reading order while preserving structure like sections, tables, lists, and equations, and doing it cheaply enough for “million-page” batch jobs. A distinctive piece is “document anchoring”: they leverage metadata/structure available in born-digital PDFs to guide extraction, and they describe structured output (JSON schema) as part of making the system robust and easier to post-process. Another novelty is evaluation philosophy: olmOCR-Bench is designed like unit tests (pass/fail rules) rather than fuzzy string matching, to make cross-system comparisons less brittle and less dependent on tokenization quirks. The paper employs RL following SFT.

### dots.ocr (25-07)
dots.ocr positions itself as a unified layout-parsing model rather than “OCR text-only”: the novelty claim is that one end-to-end pass can jointly handle layout detection, content recognition, and relational understanding (e.g., reading order) without a fragmented pipeline. A big part of the contribution is not just architecture, but the data strategy: they describe a scalable “data engine” that synthesizes high-quality multilingual training data to address the annotation bottleneck that makes multilingual document parsing hard to scale. They also explicitly push multilingual evaluation as a first-class concern by introducing XDocParse, a benchmark spanning 126 languages, and use it to support the claim that the approach generalizes beyond the typical EN/ZH-heavy evaluations.
