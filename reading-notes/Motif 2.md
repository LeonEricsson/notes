Motif 2 is a 12B dense model, upscaled from Motif-2.6B, a technique that I have not come across before. 

Model is trained on 5.5T tokens. Moun-Clip from K2 is employed.

## Post-Training
### SFT
When training Motif 12B, they argue that a static and uniform data distribution during SFT is suboptimal. This strategy forces premature convergence, preventing the model from scaling its reasoning capabilities. Instead, they argue for a dynamic shift in distribution to be essential. Motif does this in 2 stages

**Stage 1: Reasoning foundation** focuses on establishing comprehensive competence across code, mathematics, stem and tool use domains. Training corpus consists of `Nemotron-Post-Training-Dataset`, `OpenReasoningDataset` and `Mixture-of-Thoughts`. Training is done at 16-32k tokens.

**Stage 2: Deep Reasoning Specialization** targets complex inferential gaps by injecting high-granularity synthetic data including CoT intensive and failure-driven correction sets. This is done at 64k. 

Interestingly, the paper finds that the there is a high need for compatibility of reasoning traces during SFT. When comparing synthetic samples generated by either seed-oss-36b or gpt-oss-120b, the authors find that gpt-oss degrades performance against a baseline by 17 points, compared to a 12 point increase from seed-oss. "*We hypothesize that the degradation caused by gpt-oss stems from a complexity mismatch. The teacher model’s reasoning traces likely exhibit granularity and structural complexity that diverge from the student model’s intrinsic reasoning style. This discrepancy creates a distribution mismatch, where the imposed reasoning patterns conflict with the model’s learning process rather than reinforcing it*"

For the SFT recipe, Motif uses a specific data generation pipeline to generate high quality, diverse synthetic datasets engineered to inject strong reasoning signals in target domains. These datasets emphasise structured, multi-step reasoning with explicit through traces, enabling the model to internalize the logical progression required for complex problem solving. During training, they adopt a progressive length extension curriculum. 

![[Screenshot 2025-12-22 at 21.59.54.png|600]]

### RL
Motif uses GSPO, the GRPO variant with a sequence level formulation of the PPO ratio. 

#### Lessons
1. Hyperparameters for RL recipes don't transfers across SFT checkpoints. They attempted to optimize RL training recipes on intermediate checkpoints (proxy model) before applying them to the full reasoning SFT model but found that the paramers don't generalize across SFT models. 
2. Reward formulation is essential to guide policy updates, particularly the handling of format errors. They observe that output that can't be parsed should be completely discarded, and not contribute to the gradient.
3. Difficulty filtering / re-sampling / dynamic sampling is important. Too many trivial or too hard problems creates a small effective batch size and instable learning.

#### Recipe
DAPO employs dynamic sampling, Motif tries to employ a dataset filtering pipeline to construct a datset whos difficulty range is well aligned with the target model's capabilities. Specifically, they try to use the pass@k, i.e try to calculate the empirical pass rate and retain only problems who fall in a target difficulty range.

They use an extended clipping range of [0.28, 0.4].

Multi-task RL is performed, where they jointly train on all target domains - math, code, instruction following - within the RL loop. Each minibatch is a mixture of these three tasks. 