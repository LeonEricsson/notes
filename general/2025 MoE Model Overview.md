
| **Feature**          | **Total Parameters** | **Activated Parameters** | **Dense Layers** | **MoE Layers** | **MTP Layers** | **Hidden Dim** | **Dense Intermediate Dim** | **MoE Intermediate Dim** | **Granularity** | **# Experts (total)** | **# Experts Active/Token** | **# Shared Experts** | **Activation Ratio** | **Attention**                | **# Attention Heads** | **QK-Norm** | **Vocabulary Size** | **Tokens** | **Comment**                                        |
| -------------------- | -------------------- | ------------------------ | ---------------- | -------------- | -------------- | -------------- | -------------------------- | ------------------------ | --------------- | --------------------- | -------------------------- | -------------------- | -------------------- | ---------------------------- | --------------------- | ----------- | ------------------- | ---------- | -------------------------------------------------- |
| Ling mini            | 16B                  | 1.4B                     | 1                | 19             | 0              | 2048           | 5120                       | 512                      | 8.00            | 256                   | 8                          | 1                    | 3.5%                 | GQA                          | 16                    | Yes         | 157k                |            |                                                    |
| GPT-OSS              | 20B                  | 3.6B                     | 0                | 24             | 0              | 2880           | -                          | 2880                     | 2.00            | 32                    | 4                          | 0                    | 12.5%                | GQA                          | 64                    | No          | 201k                |            | Alternating SWA                                    |
| Arcee Mini           | 26B                  | 3B                       | 2                | 30             | 0              | 2048           | 6144                       | 1024                     | 4.00            | 128                   | 8                          | 1                    | 7.0%                 | GQA                          | 32                    | No          | 201k                |            | Alternating SWA 3:1                                |
| Qwen3                | 30B                  | 3B                       | 0                | 48             | 0              | 2048           | -                          | 768                      | 5.33            | 128                   | 8                          | 0                    | 6.25%                | GQA                          | 32                    | No          | 151k                |            |                                                    |
| GLM 4.7 Flash        | 30B                  | 3B                       | 1                | 47             | 1              | 2048           | 10240                      | 1536                     | 2.67            | 64                    | 4                          | 1                    | 6.25%                | MLA                          | 20                    |             | 154k                |            |                                                    |
|                      |                      |                          |                  |                |                |                |                            |                          |                 |                       |                            |                      |                      |                              |                       |             |                     |            |                                                    |
| Ling flash           | 100B                 | 6.1B                     | 1                | 31             | 0              | 4096           | 9216                       | 1024                     | 8.00            | 256                   | 8                          | 1                    | 3.5%                 | GQA                          | 32                    | Yes         | 157k                |            |                                                    |
| GLM-4.5-Air          | 106B                 | 12B                      | 1                | 45             | 1              | 4096           | 10944                      | 1408                     | 5.82            | 128                   | 8                          | 1                    | 7.0%                 | GQA                          | 96                    | No          | 151k                |            |                                                    |
| GPT-OSS              | 120B                 | 5.1B                     | 0                | 36             | 0              | 2880           | -                          | 2880                     | 2.00            | 128                   | 4                          | 0                    | 3.1%                 | GQA                          | 64                    | No          | 201k                |            | Alternating SWA                                    |
| dots.llm1            | 140B                 | 14B                      | 1                | 61             | 0              | 4096           | 10944                      | 1408                     | 5.82            | 128                   | 6                          | 2                    | 6.2%                 | MHA                          | 32                    | Yes         | 152k                |            |                                                    |
|                      |                      |                          |                  |                |                |                |                            |                          |                 |                       |                            |                      |                      |                              |                       |             |                     |            |                                                    |
| MiniMax-M2           | 230B                 | 10B                      | 0                | 62             | 1              | 3072           | -                          | 1536                     | 4.00            | 256                   | 8                          | 0                    | 3.1%                 | GQA                          | 48                    | Yes         | 200k                |            |                                                    |
| Qwen3                | 235B                 | 22B                      | 0                | 94             | 0              | 4096           | -                          | 1536                     | 5.33            | 128                   | 8                          | 0                    | 6.2%                 | GQA                          | 64                    | No          | 151k                |            |                                                    |
| Xiaomi Mimo V2 Flash | 309                  | 15                       | 1                | 48             | 1              | 4096           | -                          | 2048                     | 4               | 256                   | 8                          | 0                    | 3.1%                 | 5:1 SWA:GQA                  | 64                    | No          | 152k                |            |                                                    |
| GLM-4.{5,6,7}        | 355B                 | 32B                      | 3                | 89             | 1              | 5120           | 12288                      | 1536                     | 6.67            | 160                   | 8                          | 1                    | 5.6%                 | GQA                          | 96                    | Yes         | 151k                |            |                                                    |
| Arcee Trinity Large  | 398B                 | 13B                      | 6                | 60             | 0              | 3072           | 12288                      | 3072                     | 2               | 256                   | 4                          | 1                    | 1.9%                 | GQA 3:1 SWA                  | 48                    | Yes         | 200k                | 17T        |                                                    |
| MiniMax-M1           | 456B                 | 45.9B                    | 0                | 80             | 0              | 6144           | -                          | 9216                     | 1.33            | 32                    | 2                          | 0                    | 6.2%                 | 7:1 Lightning Attention :MHA | 64                    | No          | 200k                |            |                                                    |
| Longcat Flash        | 560B                 | ~27B                     | 0                | 28             | 1              | 6144           | 12288                      | 2048                     | 6.00            | 512                   | 12                         | 0                    | 2.3%                 | MLA                          | 64                    | No          | 131k                |            | Zero-computation experts + Short-cut connected MoE |
| DeepSeek-V3          | 671B                 | 37B                      | 3                | 58             | 1              | 7168           | 18432                      | 2048                     | 7.00            | 256                   | 8                          | 1                    | 3.1%                 | MLA                          | 128                   | No          | 129k                |            |                                                    |
| Mistral Large 3      | 675B                 | 41B                      | 3                | 58             | 1              | 7168           | 16384                      | 4096                     | 3.5             | 128                   | 4                          | 1                    | 3.1%                 | MLA?                         | 128                   | No          | 131k                |            |                                                    |
|                      |                      |                          |                  |                |                |                |                            |                          |                 |                       |                            |                      |                      |                              |                       |             |                     |            |                                                    |
| Ling 1T              | 1T                   | 50B                      | 4                | 76             | 0              | 8192           | 18432                      | 2048                     | 8.00            | 256                   | 8                          | 1                    | 3.1%                 | GQA                          | 64                    | Yes         | 157k                | 20T        |                                                    |
| Kimi K2              | 1043B                | 32B                      | 1                | 60             | 0              | 7168           | 18432                      | 2048                     | 7.00            | 384                   | 8                          | 1                    | 2.3%                 | MLA                          | 64                    | No          | 160k                | 15T        | Moun                                               |

Granularity: $2*d_\text{model} / d_\text{expert}$
Activation ratio: $(n_\text{active experts} + n_\text{shared experts}) / (n_\text{total experts} + n_\text{shared experts})$
