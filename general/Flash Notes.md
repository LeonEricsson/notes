- Building Good Language Modeling Benchmarks
	- **Naturalness**. Try to build a benchmark that has natural questions that some category of humans ask on a frequent basis. Otherwise, who is the benchmark for and why should anyone be excited about progress on the benchmark. Another way to think about fulfilling naturalness is considering usefulness: would a system that got better-than-baseline accuracy be useful to humans?
	- **Automatically evaluateable**. Certain fields have stagnated due to difficulty of evaluation, summarization is a good example of this. It is difficult to automatically evaluate a answer.
	- **Challenging**. If you launch an automatically evaluatable and natural benchmark, but the accuracy of the best LM at launch is 80%, people will see your benchmark as already being _solved_ and won’t want to try and build models to improve performance on it. Given the rapid speed of progress in the field, the "acceptable" top score on release has quickly gone from < 30% -> 0% accuracy. The author says: "_Edit, May 2025: Sorry but I have to make another edit. Due to the speed of development of AI I’m now asking my collaborators, not to think of benchmarks that would have benchmarks achieving 0% at launch, but to think of benchmarks that would have models achieving “-200%” at launch. Find questions that are so hard that even if the models improve 3x they’ll still get zero. Just building a benchmark where models get 0% today might not be enough anymore. You have to look at how the models have been improving over the past 3-6 months, try to predict where they’ll be in 6-12 months and build benchmarks that would not only make current models fail, but benchmarks that would make the models of next year fail as well. Anything easier than that might get saturated much more quickly than you expect._"
	- **Leakage**. Build benchmarks that would be hard to leak into the training data, are there benchmarks where even if the training data is exposed to the LM, it is still difficult? 
	- Have **one** number that people aim for, e.g. 80% accuracy on X. Don't use multiple metrics, don't divide accuracy by category. Just one overall number.
- Hallucination rates have steadily decreased for top models. Hallucination was something talked about a lot, say a year ago, because of the huge problem that it posed. Now a days top models have hallucinates rates of < 1%. See [the HHEM hallucination leaderboard](https://github.com/vectara/hallucination-leaderboard?tab=readme-ov-file). 
- COCONUT is notoriously expensive to train given it's sequential dependency on generating latent thoughts during training. Total forward passes becomes $\sum_{s=0}^{S_{max}-1} (D \times E_s \times ((s * c) + 1))$ where $S_{max}$ is the number of training states, $D$ the number of samples, $E_s$ the number of epochs, and $c$ the number of latent thoughts per step.  This is compared to a normal SFT which requires $D \times E$ passes. The issue comes from having to generate all the latent thoughts autoregressively before finally performing a backprop at step $n+1$ when all latent thoughts have been produced.
- Transformers are *stateless*. Transformers generate tokens $x_t$ sequentially by conditioning on the entire previously generated prefix $x_{<t}$, this prefix is re-embeded from discrete symbols. While internal computations operate on continuous hidden states, this re-evaluation of the symbolic prefix at each generation step allows for dynamic re-weighting of contextual information. This *statelessness* may allow the model to be better at sudden shifts in its thinking. As opposed to COCONUT where the transformation from $h_i$ to $h_{i+1}$ is learned as a smooth function it might be harder to represent a sudden, sharp break or a completely independent next that. 
- COCONUT's mechanism of directly chaining final hidden states as input embeddings for subsequent continuous thoughts could introduce a stronger continuity bias than standard autoregressive generation. This might make it more challenging for COCONUT to model reasoning processes that require very abrupt, discrete shifts in state if those shifts are not easily represented as a continuous transformation from the prior latent state. The "BFS-like" nature of continuous thoughts helps explore alternatives, but the transitions between these representations are still within the continuous domain.
